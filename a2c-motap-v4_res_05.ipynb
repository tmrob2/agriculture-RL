{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194f2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55421f5c",
   "metadata": {},
   "source": [
    "Wiki of CartPole-v0: https://github-wiki-see.page/m/openai/gym/wiki/CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d1d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00305335",
   "metadata": {},
   "source": [
    "The envirionment returns a reward 1 at each step. Define the (step) reward threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3810e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_rew0 = 15 # step reward threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f7289",
   "metadata": {},
   "source": [
    "Implement a task of \"achieving a position going beyond of range given by 'cart_pos'\", and define a one-off reward and a probability threshold of archieving this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a9d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_pos = 0.10 # the position for the task\n",
    "\n",
    "one_off_reward = 10 # one-off reward\n",
    "task_prob0 = 0.8 # the probability threhold of archieving the above task\n",
    "\n",
    "class PosTask():\n",
    "  def __init__(\n",
    "      self,\n",
    "      ini_status=0):\n",
    "    super().__init__()\n",
    "    self.status = ini_status\n",
    "    self.position = cart_pos\n",
    "    \n",
    "  def update(self, pos):\n",
    "    ## The two 'if' conditoins ensures to gets a score 1 \n",
    "    ## only when the target is achieved for the FIRST time.\n",
    "    if self.status == 1:\n",
    "        self.status = -1\n",
    "    \n",
    "    if abs(pos) >= self.position and self.status == 0:\n",
    "        self.status = 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "  def check(self):\n",
    "    return self.status\n",
    "\n",
    "  def reset(self):\n",
    "    self.status = 0\n",
    "    \n",
    "task = PosTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0c5da",
   "metadata": {},
   "source": [
    "Each ActorCritic network models the environment for one robot. The variables \"num_agents\" and \"num_tasks\" below define the number of robots and tasks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbce0fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 11:32:06.016560: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_agents = 2\n",
    "num_tasks = 1\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int,\n",
    "      name=None):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    ## Set \"critic\" layer dim to 'num_tasks+1'.\n",
    "    self.critic = layers.Dense(num_tasks+1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x = self.common(inputs)\n",
    "    return self.actor(x), self.critic(x)\n",
    "\n",
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "models = [ActorCritic(num_actions, num_hidden_units, name=\"AC{}\".format(i)) for i in range(num_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450d5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The reward includes a step reward and a task reward (score).\n",
    "## Warning: the number of task is hardcode into the env_step function. To improve in future.\n",
    "\n",
    "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "def env_step(state: np.ndarray, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state_new, step_reward, done, _ = env.step(action)\n",
    "    \n",
    "  ## Get a one-off reward when reaching the position threshold for the first time.   \n",
    "  task.update(state_new[0])\n",
    "  if task.check() == 1:\n",
    "    np.append(state_new, -1.0)\n",
    "    task_reward = int(one_off_reward)\n",
    "  elif task.check() == 0:\n",
    "    np.append(state_new, 0.0)\n",
    "    task_reward = 0\n",
    "  else:\n",
    "    np.append(state_new, -1.0)\n",
    "    task_reward = 0\n",
    "    \n",
    "  return (state.astype(np.float32), \n",
    "          #np.array(step_reward, np.int32),\n",
    "          np.array([step_reward, task_reward], np.int32), \n",
    "          np.array(done, np.int32))\n",
    "\n",
    "def tf_env_step(state: tf.Tensor, action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  #return tf.numpy_function(env_step, [action],\n",
    "  return tf.numpy_function(env_step, [state, action],\n",
    "                           [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6200686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "    \n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "    \n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state1 = tf.expand_dims(state, 0)\n",
    "\n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state1)\n",
    "\n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward, done = tf_env_step(state, action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "\n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "    if tf.cast(done, tf.bool):\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "  \n",
    "  ## Reset the task score at the end of each episode.\n",
    "  task.reset()\n",
    "\n",
    "  return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d71a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change \"standardize\" to \"False\", and \n",
    "## Instantiate \"discounted_sum\" to a list \"[0.0]*(num_tasks+1)\".\n",
    "\n",
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = False) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  #discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum = tf.constant([0.0]*(num_tasks+1))\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e490f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some auxiliary functions for defining the \"compute_loss\" function.\n",
    "mu = 1.0 / num_agents # fixed even probability of allocating each task to each agent \n",
    "lam = 1.0 \n",
    "chi = 1.0\n",
    "c = step_rew0  \n",
    "e = task_prob0 * one_off_reward # task reward threshold\n",
    "\n",
    "def df(x: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Threshold '<=c' is used as running rewards (not costs) are considered.\"\"\"\n",
    "  if x <= c:\n",
    "    return 2*(x-c)\n",
    "  else:\n",
    "    return tf.convert_to_tensor(0.0)\n",
    "\n",
    "def dh(x: tf.Tensor) -> tf.Tensor:\n",
    "  if x <= e:\n",
    "    return 2*(x-e)\n",
    "  else:\n",
    "    return tf.convert_to_tensor(0.0)\n",
    "\n",
    "\"\"\"\n",
    "[TO-FIX] Intend to implement the following derivative for the KL loss but get an error.\n",
    "def dh(x: tf.Tensor) -> tf.Tensor:\n",
    "  if x <= e and x > 0:\n",
    "    return tf.math.log(x/e) - tf.math.log((1-x)/(1-e))\n",
    "  else:\n",
    "    return tf.convert_to_tensor(0.0)\n",
    "\"\"\"\n",
    "\n",
    "def compute_H(X: tf.Tensor, Xi: tf.Tensor) -> tf.Tensor:\n",
    "  _, y = X.get_shape()\n",
    "  ###Try to use tf.TensorArray to implement H but get an error.!!!\n",
    "  H = [lam * df(Xi[0])]\n",
    "  for j in range(1,y):\n",
    "    H.append(chi * dh(tf.math.reduce_sum(mu * X[:,j])) * mu)\n",
    "  return tf.expand_dims(tf.convert_to_tensor(H), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9167a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The compute_loss function (with the above aux definitions) implements our loss function\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor,\n",
    "    ini_value: tf.Tensor,\n",
    "    ini_values_i: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "  #advantage = returns - values\n",
    "  H = compute_H(ini_value, ini_values_i)\n",
    "  advantage =  tf.matmul(returns - values, H)\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "    \n",
    "  #print(f'shape of action_log_probs:, {action_log_probs.get_shape()}')\n",
    "  #print(f'shape of H:, {H.get_shape()}')\n",
    "  #print(f'shape of advantage:, {advantage.get_shape()}')\n",
    "  #print(f'shape of actor_loss:, {actor_loss.get_shape()}')\n",
    "  #print(f'shape of critic_loss:, {critic_loss.get_shape()}')\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2e823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have to use a smaller learning_rate to make the training convergent\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) #0.01\n",
    "\n",
    "\n",
    "## Comment out '@tf.function'. \n",
    "## This annotation gives me an incorrect (non-reasonable) result.\n",
    "## Need to figure out whether it is possible to \n",
    "##  change our customerised implementation in order to use this feature.\n",
    "##@tf.function\n",
    "def train_step0(\n",
    "    models: List[tf.keras.Model], \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "  \n",
    "  num_model = len(models)\n",
    "  action_probs_l = []\n",
    "  values_l = []\n",
    "  rewards_l = []\n",
    "  returns_l = []\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "        \n",
    "    for i in range(num_model):\n",
    "        \n",
    "      ## add the location into the state \n",
    "      initial_state = env.reset()\n",
    "      np.append(initial_state, 0.0)\n",
    "      initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "\n",
    "      # Run the model for one episode to collect training data\n",
    "      action_probs, values, rewards = run_episode(\n",
    "        initial_state, models[i], max_steps_per_episode) \n",
    "\n",
    "      # Calculate expected returns\n",
    "      returns = get_expected_return(rewards, gamma)\n",
    "      \n",
    "      action_probs_l.append(action_probs)\n",
    "      values_l.append(values)\n",
    "      rewards_l.append(rewards)\n",
    "      returns_l.append(returns)\n",
    "    \n",
    "    ini_values = tf.convert_to_tensor([x[0,:] for x in values_l])\n",
    "    loss_l = []\n",
    "    for i in range(num_model):\n",
    "\n",
    "      # Convert training data to appropriate TF tensor shapes\n",
    "      action_probs = tf.expand_dims(action_probs_l[i], 1)\n",
    "      ## Don't need to convert the shapes of values and returns from our networks\n",
    "      # action_probs, values, returns = [\n",
    "      #  tf.expand_dims(x, 1) for x in [action_probs_l[i], values_l[i], returns_l[i]]]\n",
    "      \n",
    "      values = values_l[i]\n",
    "      returns = returns_l[i]\n",
    "\n",
    "      # Calculating loss values to update our network \n",
    "      ini_values_i = ini_values[i,:] \n",
    "      loss = compute_loss(action_probs, values, returns, ini_values, ini_values_i)\n",
    "      loss_l.append(loss)\n",
    "     \n",
    "      #print(f'ini_values for model#{i}: {ini_values_i}')\n",
    "      #print(f'loss value for model#{i}: {loss}')\n",
    "      #print(f'returns for model#{i}: {returns[0]}')\n",
    "  \n",
    "  # Compute the gradients from the loss vector\n",
    "  vars_l = [m.trainable_variables for m in models]\n",
    "  grads_l = tape.gradient(loss_l, vars_l)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  grads_l_f = [x for y in grads_l for x in y]\n",
    "  vars_l_f = [x for y in vars_l for x in y]\n",
    "  optimizer.apply_gradients(zip(grads_l_f, vars_l_f))\n",
    "\n",
    "  episode_reward_l = [tf.math.reduce_sum(rewards_l[i]) for i in range(num_agents)]\n",
    "  \n",
    "  ## For convenience, just return the first episode_reward to the console. \n",
    "  ## To improve the 'tqdm.trange' code (below) in future.\n",
    "  return episode_reward_l[0], ini_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db543b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1:   0%|             | 1/1000 [00:00<02:49,  5.88it/s, episode_reward=19, running_reward=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [-0.00256066  0.00394337]\n",
      "values at the initial state for model#1: [ 0.00505149 -0.00811187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 21:   2%|▏        | 22/1000 [00:02<01:55,  8.45it/s, episode_reward=29, running_reward=27.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [0.09545043 0.03662683]\n",
      "values at the initial state for model#1: [0.07616318 0.02120497]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 41:   4%|▍        | 42/1000 [00:05<02:12,  7.24it/s, episode_reward=49, running_reward=28.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [0.26457986 0.10558325]\n",
      "values at the initial state for model#1: [0.2251103  0.08345726]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 61:   6%|▌        | 62/1000 [00:07<01:41,  9.23it/s, episode_reward=22, running_reward=27.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [0.44333014 0.23096271]\n",
      "values at the initial state for model#1: [0.42735445 0.18248394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 81:   8%|▋        | 81/1000 [00:10<02:01,  7.58it/s, episode_reward=21, running_reward=28.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [0.6870237  0.38878995]\n",
      "values at the initial state for model#1: [0.6863609 0.3827847]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 101:  10%|▋      | 102/1000 [00:13<01:57,  7.65it/s, episode_reward=31, running_reward=28.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [0.99241644 0.6302023 ]\n",
      "values at the initial state for model#1: [1.0457394 0.5986865]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 121:  12%|▊      | 122/1000 [00:15<01:49,  8.02it/s, episode_reward=18, running_reward=29.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [1.4981639 1.0079836]\n",
      "values at the initial state for model#1: [1.5420327 0.9719183]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 141:  14%|▉      | 142/1000 [00:18<01:52,  7.63it/s, episode_reward=17, running_reward=28.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [1.9467208 1.3807025]\n",
      "values at the initial state for model#1: [2.089262  1.4075079]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 162:  16%|█▏     | 162/1000 [00:21<01:43,  8.12it/s, episode_reward=19, running_reward=29.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [2.5445607 1.8733883]\n",
      "values at the initial state for model#1: [2.639825  1.9014337]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 181:  18%|█▋       | 182/1000 [00:23<02:07,  6.40it/s, episode_reward=41, running_reward=29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [3.095605 2.366052]\n",
      "values at the initial state for model#1: [3.398965  2.4790869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 201:  20%|█▍     | 202/1000 [00:26<01:35,  8.35it/s, episode_reward=40, running_reward=29.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [3.7556157 2.9342217]\n",
      "values at the initial state for model#1: [4.1455865 3.0917428]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 221:  22%|█▌     | 222/1000 [00:28<01:37,  7.95it/s, episode_reward=17, running_reward=28.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [4.4885097 3.5957696]\n",
      "values at the initial state for model#1: [4.911976  3.7417238]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 241:  24%|█▋     | 242/1000 [00:31<01:45,  7.19it/s, episode_reward=11, running_reward=29.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [5.335581 4.339545]\n",
      "values at the initial state for model#1: [5.605027 4.406044]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 261:  26%|█▊     | 262/1000 [00:33<01:32,  7.94it/s, episode_reward=31, running_reward=28.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [5.9752126 4.844207 ]\n",
      "values at the initial state for model#1: [6.5111465 5.085073 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 281:  28%|█▉     | 282/1000 [00:36<01:27,  8.21it/s, episode_reward=16, running_reward=28.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [6.7773743 5.4806714]\n",
      "values at the initial state for model#1: [7.284358 5.725028]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 300:  30%|██     | 301/1000 [00:38<01:24,  8.23it/s, episode_reward=29, running_reward=28.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [7.642619  6.0896263]\n",
      "values at the initial state for model#1: [8.278649  6.3955555]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 321:  32%|██▎    | 322/1000 [00:41<01:20,  8.38it/s, episode_reward=28, running_reward=27.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [8.264267 6.371301]\n",
      "values at the initial state for model#1: [9.186085  6.8859615]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 341:  34%|██▍    | 342/1000 [00:44<01:27,  7.52it/s, episode_reward=47, running_reward=27.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [8.966725 6.681676]\n",
      "values at the initial state for model#1: [9.99425  7.232072]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 361:  36%|██▌    | 362/1000 [00:46<01:22,  7.78it/s, episode_reward=49, running_reward=28.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [9.927776  7.0238786]\n",
      "values at the initial state for model#1: [10.806014   7.4932666]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 382:  38%|██▋    | 383/1000 [00:49<01:04,  9.55it/s, episode_reward=12, running_reward=27.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [10.61826   7.104053]\n",
      "values at the initial state for model#1: [11.122501  7.401661]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 401:  40%|██▊    | 402/1000 [00:51<01:10,  8.52it/s, episode_reward=33, running_reward=27.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [11.174941   7.1280923]\n",
      "values at the initial state for model#1: [11.415193  7.311904]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 422:  42%|██▉    | 423/1000 [00:54<01:01,  9.43it/s, episode_reward=24, running_reward=28.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [11.82047   7.160266]\n",
      "values at the initial state for model#1: [11.891487   7.2982883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 441:  44%|███▉     | 442/1000 [00:56<01:21,  6.82it/s, episode_reward=26, running_reward=29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [12.15732    7.1057777]\n",
      "values at the initial state for model#1: [12.377342   7.3188143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 461:  46%|███▏   | 462/1000 [00:59<01:02,  8.56it/s, episode_reward=28, running_reward=28.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [12.623369   7.0621448]\n",
      "values at the initial state for model#1: [12.869357   7.2421885]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 481:  48%|███▎   | 482/1000 [01:02<01:17,  6.69it/s, episode_reward=34, running_reward=29.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.003497  7.077408]\n",
      "values at the initial state for model#1: [13.008285   7.1705284]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 501:  50%|████▌    | 502/1000 [01:04<01:02,  7.99it/s, episode_reward=24, running_reward=30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.429703  6.943088]\n",
      "values at the initial state for model#1: [13.737362   7.1550636]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 521:  52%|███▋   | 522/1000 [01:07<01:13,  6.47it/s, episode_reward=21, running_reward=29.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.667142   6.7700133]\n",
      "values at the initial state for model#1: [13.928774  7.006451]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 540:  54%|███▊   | 541/1000 [01:10<01:10,  6.48it/s, episode_reward=46, running_reward=30.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.119379   7.0164065]\n",
      "values at the initial state for model#1: [13.953038  7.049357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 561:  56%|███▉   | 562/1000 [01:14<01:05,  6.72it/s, episode_reward=21, running_reward=31.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.644855  7.36516 ]\n",
      "values at the initial state for model#1: [14.083061   7.2496037]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 581:  58%|████   | 582/1000 [01:17<01:11,  5.81it/s, episode_reward=20, running_reward=31.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.919836  7.20123 ]\n",
      "values at the initial state for model#1: [14.019248  7.027171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 600:  60%|█████▍   | 601/1000 [01:20<00:54,  7.35it/s, episode_reward=33, running_reward=32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [15.025076   6.9337616]\n",
      "values at the initial state for model#1: [14.188987   6.8432226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 621:  62%|█████▌   | 622/1000 [01:23<00:53,  7.09it/s, episode_reward=24, running_reward=32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.792558  6.810871]\n",
      "values at the initial state for model#1: [14.653498   6.9139433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 641:  64%|████▍  | 642/1000 [01:26<00:43,  8.31it/s, episode_reward=27, running_reward=30.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.602512   6.6130404]\n",
      "values at the initial state for model#1: [14.931447   6.8231187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 661:  66%|████▋  | 662/1000 [01:28<00:42,  7.87it/s, episode_reward=38, running_reward=29.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.421197  6.488619]\n",
      "values at the initial state for model#1: [15.138824   6.7897077]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 681:  68%|████▊  | 682/1000 [01:31<00:39,  8.15it/s, episode_reward=20, running_reward=28.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.290456  6.493517]\n",
      "values at the initial state for model#1: [15.160047  6.843665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 701:  70%|████▉  | 702/1000 [01:33<00:36,  8.08it/s, episode_reward=18, running_reward=27.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.067087  6.643815]\n",
      "values at the initial state for model#1: [15.164185   7.0896025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 721:  72%|█████  | 722/1000 [01:35<00:38,  7.25it/s, episode_reward=26, running_reward=27.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.875162   6.6361885]\n",
      "values at the initial state for model#1: [15.173551   7.1667314]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 741:  74%|██████▋  | 742/1000 [01:38<00:29,  8.71it/s, episode_reward=21, running_reward=27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.655541   6.5967255]\n",
      "values at the initial state for model#1: [15.106042   7.1550817]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 761:  76%|██████▊  | 762/1000 [01:40<00:28,  8.30it/s, episode_reward=20, running_reward=27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.888246   6.6174707]\n",
      "values at the initial state for model#1: [15.127593   7.0833616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 781:  78%|█████▍ | 782/1000 [01:43<00:25,  8.41it/s, episode_reward=24, running_reward=27.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.9094515  6.5291166]\n",
      "values at the initial state for model#1: [15.050762  6.992025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 801:  80%|█████▌ | 802/1000 [01:45<00:26,  7.55it/s, episode_reward=27, running_reward=27.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [13.884877   6.4417157]\n",
      "values at the initial state for model#1: [15.145258   7.0390997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 821:  82%|█████▊ | 822/1000 [01:48<00:28,  6.35it/s, episode_reward=26, running_reward=28.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.116487   6.5390687]\n",
      "values at the initial state for model#1: [15.112576  7.068258]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 841:  84%|█████▉ | 842/1000 [01:51<00:25,  6.14it/s, episode_reward=29, running_reward=29.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.393679   6.6225595]\n",
      "values at the initial state for model#1: [15.018619  6.977735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 861:  86%|██████ | 862/1000 [01:54<00:20,  6.82it/s, episode_reward=25, running_reward=29.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.478207   6.6866546]\n",
      "values at the initial state for model#1: [15.025182   7.0251617]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 881:  88%|██████▏| 882/1000 [01:57<00:16,  7.28it/s, episode_reward=49, running_reward=29.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.246804   6.4539127]\n",
      "values at the initial state for model#1: [15.014981   6.8749294]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 900:  90%|██████▎| 901/1000 [01:59<00:16,  5.91it/s, episode_reward=21, running_reward=29.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.131335  6.534855]\n",
      "values at the initial state for model#1: [14.906793   6.9316015]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 921:  92%|██████▍| 922/1000 [02:03<00:09,  7.90it/s, episode_reward=23, running_reward=29.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.4303665  6.6874723]\n",
      "values at the initial state for model#1: [14.775463  6.996707]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 940:  94%|██████▌| 941/1000 [02:05<00:09,  6.44it/s, episode_reward=59, running_reward=29.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.425964  6.68997 ]\n",
      "values at the initial state for model#1: [14.419263   6.8500514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 960:  96%|██████▋| 961/1000 [02:08<00:05,  7.58it/s, episode_reward=28, running_reward=30.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.557999  6.864701]\n",
      "values at the initial state for model#1: [14.131088  6.857766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 981:  98%|██████▊| 981/1000 [02:11<00:02,  7.06it/s, episode_reward=23, running_reward=31.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values at the initial state for model#0: [14.837031  7.10012 ]\n",
      "values at the initial state for model#1: [14.359616  7.104296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 999: 100%|██████| 1000/1000 [02:14<00:00,  7.45it/s, episode_reward=17, running_reward=30.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 999: average reward: 30.57!\n",
      "CPU times: user 2min 13s, sys: 1.57 s, total: 2min 14s\n",
      "Wall time: 2min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 1000 #10000\n",
    "max_steps_per_episode = 50 #1000\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "## No discount\n",
    "# Discount factor for future rewards\n",
    "gamma = 1.00 #0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    #initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    #episode_reward = int(train_step(\n",
    "    #    initial_state, models, optimizer, gamma, max_steps_per_episode))\n",
    "    episode_reward, ini_values = train_step0(\n",
    "        models, optimizer, gamma, max_steps_per_episode)\n",
    "    \n",
    "    episode_reward = int(episode_reward)\n",
    "\n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "    t.set_description(f'Episode {i}')\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "    # Show average episode reward every 10 episodes\n",
    "    if i % 20 == 0:\n",
    "        for k in range(num_agents):\n",
    "          print(f'values at the initial state for model#{k}: {ini_values[k]}')\n",
    "          #pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123cea8",
   "metadata": {},
   "source": [
    "### About the Experiment Outcome\n",
    "The outcome data makes sense, because we can observe the following convergeces for both models:\n",
    "\n",
    "- The two intial **step reward** cirtic values converge to a value below the threshold `step_rew0 = 15`, and \n",
    "- The two intial **task reward** cirtic values converge to a value below the threshold `task_prob0 * one_off_reward = 8`. \n",
    "\n",
    "This experiment setting use a position parameter `cart_pos = 0.10` and the maximum step per episode `max_steps_per_episode = 50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058a89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AC0\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  640       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  258       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  258       \n",
      "=================================================================\n",
      "Total params: 1,156\n",
      "Trainable params: 1,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"AC1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  640       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  258       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  258       \n",
      "=================================================================\n",
      "Total params: 1,156\n",
      "Trainable params: 1,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_agents):\n",
    "    print(models[i].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d25c360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models[0].trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "136d1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models[-1].trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3071dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython import get_ipython; get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec6804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
