import tensorflow as tf
from tensorflow import keras
import numpy as np
from typing import Tuple, List
import gym

CRITIC_LOSS_WEIGHT = 0.5
ACTOR_LOSS_WEIGHT = 1.0
ENTROPY_LOSS_WEIGHT = 0.05
BATCH_SIZE = 64
GAMMA = 1.0

huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)

def advantages(rewards, dones, values, next_value):
    """
    rewards: list of all the rewards that were accumulated during the agent's traversal of the 
    game.
    dones: list of 1 or 0 representing whether the env has finished an episode at each timestep
    values: is a list of all values V(s) generated by the model at each timestep
    next_value: is the bootstrapped estimatew of the value of all discounted rewards downstream
    of the last state recorded in the list
    """
    # create a numpy array of the list of rweards, with the bootstrapped next_value appended to it
    rewards_new = rewards[::-1]
    returns = tf.TensorArray(dtype=tf.float32, size=BATCH_SIZE)
    discounted_sum = next_value
    discounted_sum_shape = discounted_sum.shape
    type_specific_dones = tf.cast(dones, tf.float32)[::-1]
    # proceeding backwards
    for t in tf.range(1, BATCH_SIZE):
        # compute the dicounted rewards and mask by whether the episode is done or not
        reward = rewards[t]
        discounted_sum = (reward + GAMMA * discounted_sum) * (1. - type_specific_dones[t])
        discounted_sum.set_shape = discounted_sum_shape
        returns = returns.write(t, discounted_sum)
    returns = returns.stack()[::-1]
    # compute the advantages
    advs = returns - values
    return returns, advs

def compute_loss(action_probs: tf.Tensor, advantage: tf.Tensor, values, returns):
    action_log_probs = tf.math.log(action_probs)
    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)
    critic_loss = huber_loss(values, returns)
    return actor_loss + critic_loss

class EnvMem:
    def __init__(self, env: gym.Env):
        self.env = env
    def env_step(self, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        state, reward, done, _ = self.env.step(action)
        return state.astype(np.float32), np.array(reward, np.float32), np.array(done, np.int32)

    def tf_env_step(self, action: tf.Tensor) -> List[tf.Tensor]:
        return tf.numpy_function(self.env_step, [action], [tf.float32, tf.float32, tf.int32])

    def reset(self):
        return self.env.reset()
    def tf_reset(self):
        return tf.numpy_function(self.reset, [], tf.float32)





